# Web Crawling and Research System

This directory contains a comprehensive web crawling and research system designed to create detailed datasets for training AI coding agents. The system provides a structured approach to analyzing software applications through systematic website crawling, data extraction, and knowledge organization.

## Files in This Directory

### Core Documentation

**`comprehensive_crawl_research_system.md`** - The master documentation that explains the entire system, its methodology, architecture, and expected outcomes. This document provides the theoretical foundation and strategic overview of the crawling system.

**`enhanced_crawl_prompt.md`** - A detailed, systematic prompt template that guides comprehensive analysis of software applications. This is the operational document that should be used when conducting actual crawling and research activities.

**`database_schema.md`** - Defines the structure of the spreadsheet database used to store extracted information. This document specifies the nine-column schema and provides detailed descriptions of each field.

**`implementation_guidelines.md`** - Best practices and guidelines for conducting the crawling and analysis process to ensure high-quality, consistent results. This document provides practical advice for implementing the system effectively.

### Sample Data

**`sample_crawl_database.xlsx`** - An Excel spreadsheet containing eight example entries that demonstrate the expected level of detail and organization across different categories. This serves as a reference for the quality and depth of information that should be captured.

## Quick Start Guide

To use this crawling system for a new software application:

1. **Review the System Documentation** - Start with `comprehensive_crawl_research_system.md` to understand the overall approach and methodology.

2. **Customize the Crawling Prompt** - Use `enhanced_crawl_prompt.md` as your template, inserting the specific website URLs you want to analyze.

3. **Set Up Your Database** - Use the schema defined in `database_schema.md` to create your spreadsheet, or use the sample spreadsheet as a starting point.

4. **Follow Implementation Guidelines** - Refer to `implementation_guidelines.md` throughout the process to ensure quality and consistency.

5. **Reference the Sample Data** - Use `sample_crawl_database.xlsx` to understand the expected level of detail and organization.

## System Objectives

The crawling system is designed to capture comprehensive information about software applications including:

- **Features and Functionality** - What the software does and how it works
- **Architecture and Technical Implementation** - How the software is built and organized
- **User Interface and Experience** - How users interact with the software
- **Workflows and Processes** - How work gets done within the software
- **Integrations and APIs** - How the software connects with other systems
- **Design Principles and Guidelines** - The standards and patterns used in development

## Expected Outcomes

When properly implemented, this system produces a structured knowledge base that enables AI coding agents to:

- Understand software applications at multiple levels of abstraction
- Generate appropriate code that follows established patterns
- Make informed decisions about feature implementation
- Navigate codebases and documentation effectively
- Integrate with existing systems appropriately
- Maintain code quality and adhere to established standards

## Quality Standards

The system emphasizes several key quality standards:

- **Completeness** - Each entry contains sufficient detail for standalone understanding
- **Accuracy** - Technical details are verified and cross-checked
- **Consistency** - Standardized terminology and categorization throughout
- **Depth** - Captures not just functional descriptions but also design rationale
- **Context** - Provides sufficient background for informed decision-making

This crawling system represents a systematic approach to knowledge capture specifically designed for AI agent training, going beyond traditional documentation to create comprehensive training datasets.
